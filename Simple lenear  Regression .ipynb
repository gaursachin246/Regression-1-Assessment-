{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b875916-21b6-45af-bc42-5d87a334f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n",
    "\n",
    "#Difference Between Simple Linear Regression and Multiple Linear Regression:\n",
    "#Number of Independent Variables:\n",
    "\n",
    "#Simple Linear Regression: It involves only one independent variable (predictor). The relationship between this single independent variable and the dependent variable is modeled.\n",
    "#Multiple Linear Regression: It involves two or more independent variables (predictors). The model captures the relationship between the dependent variable and several independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ae8050-715a-4106-bc41-6e353ff743af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n",
    "#Assumptions of Linear Regression:\n",
    "#Linear regression, both simple and multiple, relies on several key assumptions. These assumptions ensure that the model provides reliable and valid results. Here's a discussion of each assumption and how to check if they hold in a given dataset:\n",
    "\n",
    "#Linearity:\n",
    "\n",
    "#Assumption: There is a linear relationship between the independent variables and the dependent variable. This means that the change in the dependent variable is proportional to the change in the independent variable(s).\n",
    "#Checking:\n",
    "#Scatter Plot: For simple linear regression, plot the independent variable against the dependent variable. For multiple linear regression, you can create partial regression plots.\n",
    "##Residual Plot: Plot the residuals (errors) against the predicted values. If the relationship is linear, the residuals should be randomly scattered around zero without any clear pattern.\n",
    "#Independence:\n",
    "\n",
    "#Assumption: Observations are independent of each other. In other words, the residuals (errors) should not be correlated with each other.\n",
    "#Checking:\n",
    "#Durbin-Watson Test: This statistical test detects the presence of autocorrelation in the residuals. A value close to 2 indicates no autocorrelation, while values close to 0 or 4 suggest positive or negative autocorrelation, respectively.\n",
    "#Homoscedasticity:\n",
    "\n",
    "#Assumption: The variance of the residuals (errors) is constant across all levels of the independent variables. This means the spread of the residuals should be roughly the same across all predicted values.\n",
    "#Checking:\n",
    "#Residual Plot: Plot the residuals against the predicted values. If the residuals show a funnel shape (widening or narrowing as the predicted values increase), this suggests heteroscedasticity.\n",
    "#Breusch-Pagan Test: This statistical test can also be used to check for heteroscedasticity.\n",
    "#No Multicollinearity (for Multiple Linear Regression):\n",
    "#\n",
    "#Assumption: Independent variables should not be highly correlated with each other. High multicollinearity can make it difficult to estimate the relationship between each independent variable and the dependent variable.\n",
    "#Checking:\n",
    "#Variance Inflation Factor (VIF): Calculate the VIF for each independent variable. A VIF above 10 (some use a threshold of 5) suggests high multicollinearity.\n",
    "#Correlation Matrix: Check the correlation between independent variables. High correlations (e.g., above 0.8 or 0.9) indicate potential multicollinearity issues.\n",
    "#Normality of Residuals:\n",
    "#\n",
    "#Assumption: The residuals should be normally distributed. This is particularly important for constructing confidence intervals and conducting hypothesis tests.\n",
    "#Checking:\n",
    "##Q-Q Plot: Plot the quantiles of the residuals against the quantiles of a normal distribution. If the residuals are normally distributed, the points should approximately lie on a straight line.\n",
    "#Shapiro-Wilk Test: This statistical test checks the normality of the residuals. A high p-value (>0.05) suggests that the residuals are normally distributed.\n",
    "#No Autocorrelation (for Time Series Data):\n",
    "\n",
    "#Assumption: The residuals are not correlated with each other over time. This is particularly relevant in time series data.\n",
    "#Checking:\n",
    "#Durbin-Watson Test: As mentioned, this test also checks for autocorrelation.\n",
    "#3Autocorrelation Function (ACF) Plot: This plot shows the correlation of the residuals with their lagged values. Significant spikes at non-zero lags suggest autocorrelation.\n",
    "#Summary of Checks:\n",
    "#Scatter Plot: For linearity.\n",
    "#Residual Plot: For linearity, homoscedasticity.\n",
    "#Durbin-Watson Test: For independence and autocorrelation.\n",
    "#Breusch-Pagan Test: For homoscedasticity.\n",
    "#Variance Inflation Factor (VIF): For multicollinearity.\n",
    "#Q-Q Plot and Shapiro-Wilk Test: For normality of residuals.\n",
    "#ACF Plot: For autocorrelation.\n",
    "#By ensuring these assumptions hold, the linear regression model becomes more reliable, and the inferences drawn from it are more likely to be valid.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28b2865-2fa6-48f0-9987-58bae6e5f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\n",
    "#Interpreting the Slope and Intercept in a Linear Regression Model:\n",
    "#In a linear regression model, the relationship between the dependent variable  and the independent variable is represented by the equation:\n",
    "\n",
    "# y=mx+c\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4c86c90-b0ec-4eff-a250-7cf0c9c2dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "#Concept of Gradient Descent:\n",
    "#Gradient Descent is an optimization algorithm used to minimize the cost function (or loss function) in machine learning models, particularly in linear regression, logistic regression, and neural networks. The main objective of gradient descent is to find the model parameters (such as weights and biases) that minimize the cost function, thereby improving the model's predictions.\n",
    "\n",
    "#How Gradient Descent Works:\n",
    "#Initialization:\n",
    "\n",
    "#Start with an initial guess for the parameters (e.g., weights in a linear regression model). These can be set to random values or zeros.\n",
    "#Compute the Cost Function:\n",
    "\n",
    "#Calculate the cost function, which quantifies the difference between the model's predictions and the actual target values. For linear regression, the cost function is typically the Mean Squared Error (MSE).\n",
    "#Compute the Gradient:\n",
    "\n",
    "#The gradient is a vector of partial derivatives of the cost function with respect to each parameter. It indicates the direction in which the cost function increases the most rapidly. The gradient points in the direction of the steepest ascent.\n",
    "#Update the Parameters:\n",
    "\n",
    "#Adjust the parameters in the opposite direction of the gradient to reduce the cost function. The size of these adjustments is controlled by the learning rate\n",
    "\n",
    "#Iterate:\n",
    "\n",
    "#Repeat the process of computing the gradient and updating the parameters until the cost function converges to a minimum value (i.e., further updates lead to very small or no reduction in the cost).\n",
    "\n",
    "#Convergence:\n",
    "\n",
    "#The algorithm stops when the change in the cost function between iterations is smaller than a predefined threshold, indicating that the model parameters have converged to their optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbfda5e2-5a4b-4da2-8396-4dc8e85e172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "#Multiple Linear Regression is an extension of simple linear regression, where the relationship between one dependent variable and two or more independent variables is modeled. The goal of multiple linear regression is to predict the dependent variable based on multiple predictors, and it can be represented by the following equation:\n",
    "\n",
    "#Summary:\n",
    "#Simple Linear Regression is used when there's only one independent variable influencing the dependent variable, making it simpler to interpret and understand.\n",
    "#Multiple Linear Regression allows for the inclusion of multiple independent variables, providing a more comprehensive model that can account for various factors affecting the dependent variable. However, it introduces more complexity, requiring attention to potential issues like multicollinearity and the interpretation of coefficients.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86665d6-c2b5-4185-82c5-e22c58e4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "#\n",
    "#Concept of Multicollinearity in Multiple Linear Regression:\n",
    "#Multicollinearity occurs when two or more independent variables in a multiple linear regression model are highly correlated with each other. This correlation implies that one independent variable can be linearly predicted from the others with a substantial degree of accuracy. Multicollinearity can cause several issues:\n",
    "\n",
    "#Unstable Coefficients:\n",
    "\n",
    "#The estimated coefficients of the independent variables become highly sensitive to changes in the model. A small change in the data can lead to large changes in the coefficients.\n",
    "#Difficulty in Interpretation:\n",
    "\n",
    "It becomes challenging to determine the individual effect of each independent variable on the dependent variable because the variables are not independent of each other.\n",
    "Reduced Model Reliability:\n",
    "\n",
    "Multicollinearity increases the standard errors of the coefficients, making the estimates less reliable and the model less robust.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
